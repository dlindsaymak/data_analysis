[
  {
    "objectID": "record_keeping.html",
    "href": "record_keeping.html",
    "title": "Good Record Keeping",
    "section": "",
    "text": "Keeping good notes while you are working on your studies is essential. It is a key feature to creating replicable experiments and fundamental to conducting Open Science. You should always aim for another person to be able to see where you are with your study (at any time point), be able to understand what you have done so far, and also be able to continue the study without disruption.\nOur lab is big on collaboration so create your documents with these people in mind:",
    "crumbs": [
      "Record Keeping"
    ]
  },
  {
    "objectID": "record_keeping.html#before-analysis",
    "href": "record_keeping.html#before-analysis",
    "title": "Good Record Keeping",
    "section": "Before Analysis",
    "text": "Before Analysis\nBefore you even start analyzing data you want to be thinking about keeping good records. While you are developing your study you will have several clear documents that you will refer back to during analysis. These include:\n\nA spreadsheet of all your variables/items \n\nDetails where they came from, how you plan to use them in your analysis, and any notes on these items\n\nAn Analysis Plan\n\nExactly how you will deal with your data once you have collected it\n\n\nCreating your analysis plan is discussed below. Creating the spreadsheet will be discussed in another guide.\nOnce you have your data and you are cleaning it up (see the instructions below) you should be keeping notes of literally everything you do. If you decide to clean your data in Excel, then create a text document that records everything you do to that data so you can replicate it. If you use R (which I recommend) you can keep notes using # that describes what every piece of code means. I’m always of the opinion that more notes are better. It doesn’t matter if it seems redundant, add the note because you (or another person) will be grateful for those notes later.",
    "crumbs": [
      "Record Keeping"
    ]
  },
  {
    "objectID": "record_keeping.html#during-analysis",
    "href": "record_keeping.html#during-analysis",
    "title": "Good Record Keeping",
    "section": "During Analysis",
    "text": "During Analysis\nEvery step of the analysis process needs to be recorded. As with your data cleaning, make notes of every analysis you do and the results of these analyses. \nI recommend creating an Analysis Document (example here) that is based on your Analysis Plan and records everything you do during your analysis. This way you can refer back to this when you are writing up your results to see exactly what you did at what step.\nSimilarly, keep good notes in your R script for your analyses. If you have lots of different analyses to run I recommend creating different R script files to make it easier to locate (e.g. Preliminary analysis, hypothesis testing, exploratory analysis, factor analyses). But make sure you note if you need to run any other scripts for that particular script to run. Add this to the top of your R script so you quickly see what needs to be done. This way you aren’t pulling your hair out wondering why your script isn’t running.\nAn example of the notes at the top of one of my R scripts:\n# Exploratory Analyses for Choice vs. Compliance Study\n# Run the following scripts before working on this code:\n# CvC_cleaning.R\n# CvC_prelim.R\n\n# Make sure the following packages are installed\n\n#install.packages('tidyverse') \n#install.packages('Hmisc') #for correlations \n#install.packages('emmeans') #estimated means",
    "crumbs": [
      "Record Keeping"
    ]
  },
  {
    "objectID": "record_keeping.html#after-analysis",
    "href": "record_keeping.html#after-analysis",
    "title": "Good Record Keeping",
    "section": "After Analysis",
    "text": "After Analysis\nEven after analysis you’ll have record keeping that needs to be updated. You’ll inevitably get feedback on your results in lab meetings that you’ll need to note. Keep these in your analysis document under a section called “to be looked at” or similar. \nYou should also include a brief write up of your analyses in your Analysis document so you can easily refer to them when writing a paper or presenting results from your study.",
    "crumbs": [
      "Record Keeping"
    ]
  },
  {
    "objectID": "downloading.html",
    "href": "downloading.html",
    "title": "Downloading Data from Qualtrics",
    "section": "",
    "text": "The very first step to analyzing your data is getting that data out of Qualtrics. I’m going to link the Qualtrics help page about downloading data HERE in case they’ve changed things since I wrote this guide, but here are the steps currently.\n\nLog into your Qualtrics account\nNavigate to the survey you want to export your response from\nClick on the “Data & Analysis” tab on the top menu\n\nClick “Export & Import”, then “Export Data”\n\n\nYou will now get a pop-up that gives you a bunch of options on how you want your data downloaded.\n\nFile Type: You want to export as a CSV. This is the most commonly used data document and you can then open it in many programs.\nDownload all fields: Yes, do this \nUse numeric values: This is ideal, it will make sure you can analyze your likert scales. This records responses as numbers (e.g., 1 instead of “strongly disagree”)\n\nNOTE: Make sure you have coded your responses correctly in Qualtrics otherwise your numbers could be weird (especially if you have done a lot of editing in your likert scales). \n\nClick open the More Options window and you might want to select the following:\n\nCompress data as .zip file\n\nUseful if you have a VERY BIG data set and don’t want to wait forever for it to download\n\nExport viewing order data for randomized surveys\n\nThis will be relevant if you need to code for order of presentation and you do not have any embedded data that keeps track of that\n\nSplit multi-value fields into columns\n\nThis is the default and you should keep it selected. Especially if you have matrix questions with multiple items on them.\n\nUse internal IDs in header\n\nDO NOT Check this. This gives you the Qualtrics question id’s as the heading which are completely meaningless and will make running your analysis really difficult!\n\n\n\n\nQualtrics will give your file a useful name based on the title of the survey and the date you downloaded it. You might want to rename it to make it shorter/easier for you (especially with uploading to R later on). But I recommend keeping the following details:\n\nA recognisable study name (e.g. ChoiceVCompliance)\nWhere the participants are coming from. Especially important if you plan on collecting from different populations (e.g. ChoiceVComplianceSONA)\nWhether this is part 1 or part 2 of a study (e.g. CharStrength_Part1)\nThe date you downloaded the data. This is super important if you are downloading an active study that is still collecting data. (e.g. CharStrength_Part1_Jan_19_2022) \n\nYou’ll need to download all the surveys you need for your study. So, if you have a multi-part study, make sure you have every piece of data from every survey you issued. You’ll piece them all together during data cleaning later, but make sure you have them all downloaded, named clearly, and filed together first.\nMake sure to save your data in a place you can find it again. You can always re-download from Qualtrics, but it helps to know where your data is. Organization makes data analysis 100x easier.\n\n\n\nDon’t do this\n\n\nNow that you have your data we can think about getting it ready for analysis…",
    "crumbs": [
      "Downloading from Qualtics"
    ]
  },
  {
    "objectID": "analysis_plan.html",
    "href": "analysis_plan.html",
    "title": "Having an Analysis Plan",
    "section": "",
    "text": "Similar to Good Record Keeping, having an Analysis Plan is a critical component of Open Science. This will be something you often do before you start collecting data. An example Analysis Plan is here.\nIdeally you will have all your analyses planned out before your data collection is finished so you can just run through the analyses without spending time wondering what it is you have to do to answer your research questions. \nYour Analysis Plan will function like a to-do list that you follow. Each Analysis Plan will vary based on the study you are doing but it will composed of four sections:\n\nData Cleaning\n\nBasic Cleaning\nExclusions\n\nPreliminary Data Analyses\n\nCheck assumptions needed for Hypothesis Testing\nDistribution of variables across conditions \nDescriptive Statistics\n\nHypothesis Testing\nExploratory Analyses\n\n\nData Cleaning\nThis is everything you do to the data before you run any analyses on it.\n\nBasic Data Cleaning\nYou want to make a list here of all the things you need to do with your data to get it ready for analysis. Things like\n\nTidying up the labels\nCombining data from different sources\nMaking sure the values are coded correctly\nMaking sure your reversed items are scored correctly\nRemoving your test subjects and any technical issues \nCalculating difference scores between pre and post tests\n\nThink ahead of everything you’ll need to do to your data so it can be used. You can also add to this while you are collecting data if something comes up (e.g. a fire alarm goes off during one of your experiment sessions so you need to remove all the participants that were doing the study at that time).\n\n\nExclusions\nMake a note in your analysis plan of your exclusion criteria. These are participants you will not include in the analyses you run. Common exclusion criteria are:\n\nAttrition (not finishing the study)\nFailing attention checks (be clear about your cut off for failing these checks)\nDetecting the purpose of a study that includes deception\nHaving participated in a similar activity before\nNot currently being located in a particular state or country\n\nYour exclusion criteria will be specific to your study, but you should make note of your decision and why you are deciding to not include these participants in your Analysis Plan.\n\n\n\nPreliminary Data Analysis\nThese are basic analyses that you need to run before you test your main hypotheses. \n\nAssumption Checking\nIn order for your hypothesis tests to be sound you need to show that your data met certain assumptions. You will need to report these in your write up so make sure you have a note of what you need to run depending on the types of tests you are planning.\nThese are things like:\n\nTests for normality\nTest for homogeneity of variance\n\n\n\nDistribution of Variables across Conditions\nThis is similar to assumption checking, in that you need to make sure that your samples in each condition are relatively evenly distributed across variables that might act as confounds. \nThings to check here are:\n\nDemographics\nAny pre-test measures\n\nYou might have other things that will matter if they aren’t balanced, so make sure you think about this in your Analysis Plan.\nNote what you are planning on running and what type of test you will need (chi-square for categorical or ANOVA for continuous).\n\n\nDescriptive Statistics\nYou will also need to report the basic descriptive statistics for your sample. Make a plan of all the descriptives you will need to run and what type of tests they require chi-square for categorical or ANOVA for continuous).\n\n\n\nHypothesis Testing\nThis is where you will plan out what type of tests you will need to run to answer your research question/s. You should have a clear hypothesis for each question and you should then convert that into a statistical test that will test if that hypothesis is statistically probable.\nOrganize this section based on each of your hypotheses and make a note of what tests you will need to run.\n\n\nExploratory Analyses\nUse an Exploratory Analyses section to make a note of any other analyses you think will be interesting to run but aren’t relevant to answer your main hypotheses.",
    "crumbs": [
      "Analysis Plan"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "cleaning.html",
    "href": "cleaning.html",
    "title": "Cleaning Data",
    "section": "",
    "text": "I’m not going to lie to you here. Data cleaning is the BIGGEST step in data analysis. You literally cannot do anything until you get your data into a format that can be used. And this takes time. \nYou can clean your data in many ways using many programs. But however you do it, you always want to be able to \n\nSee your work, and\nGo back to an earlier stage to check/edit any changes you’ve made\n\nThis requires very good note taking, version control, and/or change tracking. (Remember what I wrote in Good Record Keeping? That is all very relevant now)\nI prefer to do all my data cleaning in R because this allows me to do these things. Excel is easier, but it’s very easy to mess up your data and not know where you messed up. R is more difficult to get the hang of, but (if you do it right) you can clearly see what you have done every step of the way and mistakes are easy to find and change. \nHere are my hard and fast rules on data cleaning…",
    "crumbs": [
      "Data Cleaning"
    ]
  },
  {
    "objectID": "cleaning.html#aka-getting-data-ready-for-analysis",
    "href": "cleaning.html#aka-getting-data-ready-for-analysis",
    "title": "Cleaning Data",
    "section": "",
    "text": "I’m not going to lie to you here. Data cleaning is the BIGGEST step in data analysis. You literally cannot do anything until you get your data into a format that can be used. And this takes time. \nYou can clean your data in many ways using many programs. But however you do it, you always want to be able to \n\nSee your work, and\nGo back to an earlier stage to check/edit any changes you’ve made\n\nThis requires very good note taking, version control, and/or change tracking. (Remember what I wrote in Good Record Keeping? That is all very relevant now)\nI prefer to do all my data cleaning in R because this allows me to do these things. Excel is easier, but it’s very easy to mess up your data and not know where you messed up. R is more difficult to get the hang of, but (if you do it right) you can clearly see what you have done every step of the way and mistakes are easy to find and change. \nHere are my hard and fast rules on data cleaning…",
    "crumbs": [
      "Data Cleaning"
    ]
  },
  {
    "objectID": "cleaning.html#rules-of-data-cleaning",
    "href": "cleaning.html#rules-of-data-cleaning",
    "title": "Cleaning Data",
    "section": "Rules of Data Cleaning",
    "text": "Rules of Data Cleaning\n\nNever EVER edit the raw data set\nYes you can always re-download your data from Qualtrics, but don’t rely on that being there. You want to have the purest version of your data always available. This allows you to share it with other researchers and allows you to always have the ability to go back to square one if things go sideways (which can happen during cleaning).\n\nKeep versions of all major changes to your data\nThink of this like re-spawn places. How much work do you want to have to re-do if you find a big issue and need to start over again? My go-to major changes are\n\nCombining Pre and Post surveys\nRestructuring from a long to wide data (This is a much more technical point that I’ll cover in another guide)\nAdding new columns that aggregate new data points\n\nThis will likely be different for every project, but it’s exceptionally helpful to have these versions (especially if you are working in Excel).\n\nMake notes of ALL changes\nAnd I mean ALL the changes you make. My R cleaning files are fully annotated with notes about each change I make to the data and why I made those changes. I was once told that you should leave notes in your analysis documents that are good enough that someone else could look at your code and understand what is going on – and usually that “someone else” is you in 6 months time. I guarantee you that you will not remember what those changes were when you come back to your project after a couple of weeks of doing something else. And if you intend to go further in research (like grad school), you will definitely have data that you haven’t looked at for 2-3 years and will need those detailed notes. Make. Lots. Of. Notes.\n\nOkay, now we can get to the meat of the exercise. Every project will have specific data cleaning needs but there are a few that are common enough that I’m going to cover them here. These are as follows…",
    "crumbs": [
      "Data Cleaning"
    ]
  },
  {
    "objectID": "cleaning.html#common-data-cleaning-tasks",
    "href": "cleaning.html#common-data-cleaning-tasks",
    "title": "Cleaning Data",
    "section": "Common Data Cleaning Tasks",
    "text": "Common Data Cleaning Tasks\n\nGetting Started With R\nRemoving irrelevant data (e.g. test subjects, technical errors etc)\nCleaning up your labels\nCombining surveys by participant ID (for multi-part studies)\nExclusions \nAttention checks\nEnsuring responses can be analyzed appropriately\nCreating aggregate scores (for specific measures)\n\n\nGetting Started With R\n\n\n\n\n\n\nDownload R & R Studio\nYou need both. R runs the code, R Studio is easier to work with than R alone.\n\nDownloading R\n\nClick on CRAN (under downloads) on the R-project site\nUse the first server to download - it’s called 0-Cloud\nSelect the R download for the system you are using (Windows, macOS, or Linux)\nI typically recommend downloading the latest version, but check to make sure you are running the appropriate OS to handle it\nInstall based on your OS\n\n\n\nDownloading R Studio\n\nScroll down on the R Studio site until you see either a button for downloading for Mac, or a bunch of links for downloading for other OS’s\nInstall based on your OS\n\nYou will code in R Studio. This program is basically an overlay for R that makes it more user-friendly. \n\n\n\n\nExtra R Resources\nTo begin with I’m going to recommend R for Data Science as a great resource for cleaning your data in R. This is what I used to learn a lot of R and the packages I use to clean are detailed by them (e.g. Tidyverse). I would recommend bookmarking this e-book for future reference.\n-&gt; https://r4ds.had.co.nz/ \nThere’s also some really good courses on CodeAcademy and Coursera that cover basic R. It’s definitely helpful to start playing around with R now to get comfortable with the format. \nUseful course links:\n\nhttps://www.codecademy.com/catalog/language/r\nhttps://www.coursera.org/projects/getting-started-with-r\nhttps://ldierker1.github.io/passiondrivenstatistics/\n\n\n\n\nCreating a Project\nThe first thing you need to do in R is create a project. I’m not going to reinvent the wheel here. The R Studio help site has everything you need to create a project. Check their instructions out here - https://support.rstudio.com/hc/en-us/articles/200526207-Using-RStudio-Projects\n\n\nSetting up Your Cleaning File\nOnce you have created your project, set your cleaning script up as follows:\n\nCreate a new R Script File and give it a meaningful name related to “Cleaning”. You will use this one script to clean all your data. This makes it easier to clean new data for your project when it comes in (e.g. when you run future phases of data collection)\n\nMake sure the Tidyverse package is installed first. Type the following code in your Cleaning Script:\n#this installs the packages \ninstall.packages(\"tidyverse\") \ninstall.packages(“lubridate”)\n\n#this runs the package \nlibrary(tidyverse) \nlibrary(lubridate)\n\n\n\nMake a note of when you downloaded the data. This will allow you to refer quickly to what data set you are analyzing. This is really important if you are running analyses before you have collected all the  data. I like to make notes about whether I am still collecting data so I know where I’m at.\n\n#Data downloaded from Qualtrics Thursday February 10 2022\n#Data collection is still ongoing for this project\n\nGive a useful title and make any notes at the start of your cleaning document. Things to include might be:\n\nThis chunk of cleaning script is related to the Pre Survey/Post Survey\nIf you have made any changes in Excel before you imported into R\nAnything weird you think you’ll need to remember in the future\n\n\n###################################################\n#Cleaning up the Pre-Survey\n#Two rows removed in Excel before I imported into R\n###################################################\n\nLoad data & Save it to a data frame\n\nTo do this you are going to give your data frame a useful name. You can name this whatever you want, but make sure it makes sense. For full instructions on importing data see https://r4ds.had.co.nz/data-import.html.\nAs a note, I save my R files in a very specific file structure:\n\nProject_name\n\nScript\n\ncleaning.r\nprelim_analyses.r\nhypothesis_testing.r\n\nData\n\nproject_pre_date.csv\nproject_post_date.csv\n\nArchive\n\nAll old data and scripts not being currently used\n\n\n\nBecause of this, I need to use the prefix ./Data/ before I call the csv I am importing. If you save your data directly into the same folder as your cleaning script you can just type out the csv inside the “ “\ndata &lt;- \n  read_csv(\"./Data/CS_P3_Pre_Feb102022csv.csv\", na = \"\")\nOnce you load your data set, you can open it to see what it looks like. Either click on the data in the Environment window on the right, or type the following into your Console at the bottom of your screen: View(data)\nThis will open a window of your data and is really helpful to give you a sense of what your data looks like:\n\n\n\n\n\nAt the top there is the heading row which will automatically be the first row that R imported. This is why it is important to give your Qualtrics questions meaningful labels - so you can figure them out later on in R.\nWhen you import from Qualtrics you will always get a spare row that describes what that column is. This is the text that is displayed in the question and you’ll want to delete it straight away as you don’t need it in R.\nThen it will show you a row for each submission in Qualtrics. \nQualtrics also includes a bunch of unnecessary metadata at the start of your spreadsheet that you can get rid of. Some of this is useful (progress, recordedDate can sometimes be helpful) but ultimately we are going to delete these columns. \nNow we can get started on actually cleaning…\n\n\n\nRemoving irrelevant data (e.g. test subjects, technical errors etc)\n\nRemove the extra row/s from Qualtrics \nAs I said earlier, Qualtrics puts extra rows in your spreadsheet with more descriptive text about what is being imported. You can remove this in excel before you import into R - make sure to make a note of that in your R code so you know you’ve done this. Or you can remove it in R using this code:\ndata &lt;- \n  data[-(1:2),] \n\n# Code Notes: # Using [ ] indexes the rows by row number \n# The - sign tells R to remove them (1:2) means everything between rows 1 and 2 \n# The , with nothing after it indicates that we don't care about columns\n\n\nRemoving Test Subjects\nYou are likely going to have a bunch of test data at the start of the sheet. If you know when your study actually went live you can remove test subjects using that date. Otherwise, you might need to look for other tells\n\nThe word “test” in a specific field \nNot including a proper unique ID (if your RA’s manually enter these - typical for in-lab studies)\n\nYou can remove your test subjects in Excel before you import into R, but make sure you make a note of it when you import your data.\nTo remove them by date in R you need to do a bit of wrangling first. The first step is to make sure the startDate is read as an actual date. For more information about dates in the Tidyverse see - https://r4ds.had.co.nz/dates-and-times.html#dates-and-times \nRemember to always start with a note of what you are doing\nYou can use the same data-frame name to save over here. (I typically do this until I’m making big data-frame changes)\nUse filter() to find people who started after the survey went live.\nFor full instructions on using filter see - https://r4ds.had.co.nz/transform.html#filter-rows-with-filter \n# Remove test subjects - anything that was recorded before January 31 2022, 11.59am\n# Make sure Start Date is read as a date\n\ndata &lt;-\n  data %&gt;% \n  mutate(StartDate = mdy_hm(StartDate, tz = \"America/Los_Angeles\"))\n# Note: you need to call the package \"lubridate\" for this to work\n# It is also important to make sure the time zone is set to the right time so when you remove participants they will be in relation to the time YOU actually started the study (not their local time)\n# to see all the timezones that you might need use: OlsonNames()\n\n\nstr(data$StartDate) #checking the structure of the date\n\n# The StartDate variable will now show as POSIxct, which is the code R uses for a unicode data\n\n&gt; str(data$StartDate)\n POSIXct[1:583], format: \"2022-11-08 14:30:10\" \"2022-11-08 14:31:39\" \"2022-11-08 14:37:51\" ...\n\n\ndata &lt;-\n  data %&gt;% \n  filter(StartDate &gt; \"2022-01-31 11:59:00 UTC\")\n# Note: You'll need to place your date/time in \" \" for R to read it and make sure to end with UTC\n\n\nRemoving Technical Errors\nYou may also have technical errors that you know you have to remove. You should have a note document for your study that keeps track of any issues while the study is being run. If you need to remove participants because of technical issues while they completed the study you can remove them by their ID. \nThe actual label that is used for their ID will be different for each study, so make sure you know what you labeled it.\nI use the Filter function to remove rows that have certain ID’s. \n#Remove subjects 3CS5643 and 1CS45793 because of problems with the laptop while running study\n\ndata &lt;-\n    data %&gt;%\n    filter(unique_id != \"3CS5643\" & unique_id != \"1CS45793\")\n# Code notes:\n#       != means 'does not equal\n#       put the id's in \" \" because they will be character strings and they need \" \" for R to find them\n#       & means AND, so R will remove rows that have the ID as 3CS5643 AND rows that have the ID as 1CS45793 \n#       make sure you specify the label unique_id again for each entry after an AND (&) otherwise it won't work\n\n\nRemoving Unnecessary Columns\nQualtrics is also going to put a lot of columns in that you don’t need. Most of these are meta-data that you will never use so you can get rid of them to make viewing your sheet easier. It’s not necessary to remove them, but I have a rule of only keeping data that is relevant to my analysis.\nYou can delete these in Excel before you import - remember to make a note in R if you do this.\nTo remove in R, use the select() function to remove all the rows you don’t need. Placing a minus sign (-) in front of each variable will remove it.\nI like to leave in \n\nprogress\nduration\n\ndata &lt;-\n    data %&gt;%  \n    select(-StartDate, \n        -EndDate,\n            -Status, \n            .... ) \n\n# Coding notes:\n# Select allows us to select certain columns, \n# Adding a - sign removes them\n\n\n\nCleaning up your labels \nIf you haven’t labelled your items sensibly in Qualtrics before you start you may have to do some editing now so you can understand your data. You can also do this in Excel, but remember to make a note of this when you import your data into R.\nTo see all the variable names in your spreadsheet easily use this code: variable.names(data)\nAll the names will then print in the Console at the bottom of your R screen:\n\n\n\n\n\nI’ve labelled my items based on what measure they are:\n\nWB = Ryff Well-being Scale\nCES_D = Depression Inventory\nAtt = Attention check items\n\nIf you need to change any variable names use the following:\n#Rename duration column\ndata &lt;-\n  data %&gt;% \n  rename(duration = `Duration (in seconds)`)\n\n# Code Notes:\n# rename is in the same package as select()\n# It functions as... rename(new_name = old_name)\n\n\nCombining surveys by participant ID (for multi-part studies)\nIf you have multi-part studies then you will need to combine the separate spreadsheets together so you can analyze them as one spreadsheet. \nThis is done by having a unique ID for each participant that they enter for each survey so you can match them across surveys.\nYou will want to tidy up each spreadsheet before you combine them together. So, make sure you go through the steps above for each spreadsheet before you get to this step.\nThere are some important things to note when you merge spreadsheets\n\nMake sure there is a participant ID on each data frame\nMake sure the label/variable name for participant ID is the same on each data frame\n\nThe code you will need to this is:\n#create a new dataframe for all the data\ndata_all &lt;-\n  full_join(data_pre, data_post, by= \"id\", copy = TRUE, suffix = c(\"_pre\", \"_post\"))\n\n# Code Notes:\n# I use full_join() because I want to keep everything from both data frames\n# The first two terms are the two dataframes that you are merging\n# by = tells R what column you are using as the key to merge across the frames\n# I use \"copy = TRUE\" because I want duplicates of my variables \n# And R will append the suffixes _pre and _post to the duplicates it finds \nThere are several ways to merge data frames together. See this section of R for Data Science for a good overview - https://r4ds.had.co.nz/relational-data.html#relational-data\nIf you have more data frames to join you can pipe them on to the code as follows:\n#create a new dataframe for all the data\ndata_all &lt;-\n  full_join(data_pre, data_post, by= \"id\", copy = TRUE, suffix = c(\"_pre\", \"_post\")) %&gt;% \n  full_join(data_day1, by = \"id\", copy = TRUE) %&gt;% \n  full_join(data_day2, by = \"id\", copy = TRUE) %&gt;% \n  full_join(data_day3, by = \"id\", copy = TRUE) %&gt;% \n  full_join(data_day4, by = \"id\", copy = TRUE) %&gt;% \n  full_join(data_day5, by = \"id\", copy = TRUE) %&gt;% \n  full_join(data_day6, by = \"id\", copy = TRUE) %&gt;% \n  full_join(data_day7, by = \"id\", copy = TRUE)\nAnd now you should have one giant data frame that has a row for each of your participants’ responses across all surveys!\nNow is a good time to take a look at the data and see if you notice anything weird:\n\nUnusual id’s that don’t fit the pattern you created\nAn entire row of missing data\nExtraordinarily long progress times\n\nThere may be some things you need to clean up and address at this stage, but everything we’ve covered should help you with that. If you are still lost on how to fix your problem, reach out to whoever is mentoring you.\n\n\nExclusions\n\n\n\n\n\nParticipants will need to be excluded from your study. You will have made some a priori decisions about your exclusion criteria and that is what you will follow in this section. Common exclusions are:\n\nAttrition - removing those who didn’t complete the whole study\nHeard about the details of the study\nFigured out the deception being used\n\n\nAttrition\nEach study will have different rules for removing participants who did not complete the whole study. If you have one survey you might only include people who have 100% progress. If you have multiple parts to your study, you might exclude participants who do not return for part 2. Check with the exclusion rules for your study and remove participants who meet the attrition exclusion requirements.\nQualtrics provides a progress variable that is useful at filtering participants at this stage. \nUse the filter() function to remove participants where their progress does not equal 100.\n# remove participants who did not complete all of the Pre survey\n\ndata_clean &lt;-\n  data_all %&gt;% \n  filter(Progress_pre == 100)\nBe sure to keep a record of where people dropped out of the study. If you have multiple surveys that they need to complete (e.g. daily logging surveys for an intervention) you can count the number of participants who completed each day with the following:\n# Count where participants dropped off during the weekly intervention\n\n#day 1\ndata_clean %&gt;% \n  filter(!is.na(Progress_1)) %&gt;% \n  nrow()\n\n#day 2\ndata_clean %&gt;% \n  filter(!is.na(Progress_2)) %&gt;% \n  nrow()\n\n#day 3\ndata_clean %&gt;% \n  filter(!is.na(Progress_3)) %&gt;% \n  nrow()\n\n#day 4\ndata_clean %&gt;% \n  filter(!is.na(Progress_4)) %&gt;% \n  nrow()\nThis gives you counts of each day:\n\n\n\n\n\n\n\nHeard about the details of the study\nIf you have a question that asks if the participant heard about the details of the study then you can use this to filter them out.\nYou may need to double check in Qualtrics what each number relates to (e.g., 1 = Yes; 2 = No)\n# Remove participants who said they heard about the details of the study\n# 1 = Yes; 2 = No \n\ndata_clean &lt;-\n  data_clean %&gt;% \n  filter(hear_about_post == 2) #only include those who said they didn't hear (2)\nMake a note of how many participants you remove at this stage\n\n\nFigured out deception\nIf you have deception in your study you may have decided to remove participants who correctly guessed the purpose of the study.\nYou will need to read through the free responses and decide if anyone has to be removed. This is often easier in Excel.\nThen remove them based on their ID.\n# remove participants ID 2CS45673 because they guessed the purpose of the study\ndata_clean &lt;-\n  data_clean %&gt;% \n  filter(id != \"2CS45673\")\n\n\n\nAttention checks\nOur lab uses a standard set of attention and integrity checks to ensure the integrity of our self-report data. As part of your exclusion criteria you will have made some a priori decisions on who to include in your study. Use these as your rules when filtering out participants.\nThere’s a few steps in dealing with your attention checks. \n\nFirst you’ll need to combine them to make an overall proportion score of how each participant did. This is because you’ve probably got an exclusion rule based on how many checks a participant can miss. \nYou may also need to account for different attention check questions appearing in different conditions. You don’t want to exclude a participant for missing an attention check that wasn’t even shown to them because they weren’t in that condition. \nIf you have an intervention or exercise that requires sustained attention on a video, audio file, or activity outside of a survey, you will need to separate out these integrity questions from the attention checks.\nFinally, you may have to retain some people in an intent-to-treat dataset. This allows for checking if simply being in a study (but not really paying attention to it) led to any improvements.\n\n\nCombining Attention Checks\nI always start by making a note of all the attention checks and integrity questions that were included in the study and what the a priori decision was regarding exclusions.\n#objective attention checks\n#   all participants\n#       att_1_pre -&gt; correct response == 6\n#       att_2_pre -&gt; correct response == 1\n#       att_1_post -&gt; correct response == 2\n#       att_2_post -&gt; correct response == 2\n#   strength conditions only\n#       att_3_pre -&gt; correct response == 3\n#       att_4_pre -&gt; correct response == 2\nI now use the ifelse() function to allocate a 0 or a 1 to a participant if they failed or passed the attention check. I use this with the Mutate() function to save this number over their responses.\nIfelse works like this:\nIfelse (variable_name [does not equal] value, if true then place this value, if false then place this value)\nFor example: \nIfelse(att_1_pre != 6, 0, 1)\nIf the value in att_1_pre does not equal 6 then put a 0 (because they failed the attention check), if it does equal 6 then put a 1\nThe example of code below also includes if else conditions for the total attention score which is what you will use if you have different conditions with different attention checks visible. It’s the same logic as above:\nifelse(condition == “top” | condition == “bottom”, (att_1_pre + att_2_pre + att_1_post + att_2_post + att_3_pre + att_4_pre), (att_1_pre + att_2_pre + att_1_post + att_2_post)\nIf the condition equals top OR bottom, then add up these variables (because these are the attention checks those participants saw. If the condition is not one of those two, then just add up these variables (because they only saw those checks)\nThen I mutate a final variable that calculates a proportion (from 0-1) so I can easily see how many attention checks were passed\n#create a total attention check percentage \n# 0 = failed, 1 = passed\ndata_clean &lt;-\n  data_clean %&gt;% \n  mutate(att_1_pre = ifelse(att_1_pre != 6, 0, 1),\n         att_2_pre = ifelse(att_2_pre != 1, 0, 1),\n         att_1_post = ifelse(att_1_post != 2, 0, 1),\n         att_2_post = ifelse(att_2_post != 2, 0, 1),\n         att_3_pre = ifelse(att_3_pre != 3, 0, 1),\n         att_4_pre = ifelse(att_4_pre != 2, 0, 1),\n         att_total = ifelse(condition_pre == \"top\" | \n                              condition_pre == \"bottom\", \n                            (att_1_pre + att_2_pre + att_1_post + att_2_post + att_3_pre + att_4_pre), \n                            (att_1_pre + att_2_pre + att_1_post + att_2_post)),\n         att_pc = ifelse(condition_pre == \"top\" | \n                           condition_pre == \"bottom\", \n                         round((att_total / 6), 2),\n                         round((att_total / 4),2)\n         )) \nIf you have multiple study parts you may also need to do the same aggregation with the subjective attention check.\n\n\nFilter out Failed Attention Checks\nNow you can filter out the participants who do not meet your criteria. Remember to make a note of what this is in your R file. And I always do another count so I can keep track of how many participants were dropped because of failing attention checks\n#Keep participants who:\n#     have .75 or more on att_pc\n#     AND have exactly 1 on sub_att_prop\n\ndata_clean &lt;-\n  data_clean %&gt;% \n  filter(att_pc &gt;= .75 & sub_att_prop == 1)\n\n#count participants for records\nnrow(data_clean)\n\n\nIntent to Treat (Intervention Studies)\nIf you have an intervention in your study and asked the question about how much people engaged during this activity then you will also need to create a new variable that you can use to run intent to treat analyses.\n#create intent to treat item - 1 included in main analysis, 0 included in intent to treat analysis\n#integrity check (all participants)\n#   integrity (keep 1 and 2)\ndata_clean &lt;-\n  data_clean %&gt;% \n  mutate(intent_treat = ifelse(integrity == 1 | integrity == 2, 1, 0))\n\n\n\nEnsuring responses can be analyzed appropriately\n\n\n\n\n\nWhen you started your project you should have created a spreadsheet of all the variables and what you plan to use them for in your analysis - see example here. Using this spreadsheet you should check all your variables to make sure they are recorded as the correct type (e.g. numeric, factor, character)\nThe main types of variables you will be using are:\n\nNumeric values \n\nAny number value (including scores on a Likert scale) that you will use for regressions, correlations, factor analyses etc\nContinuous variables needs to be recorded as numeric\nIn R this is noted as double (dbl)\n\nFactor values\n\nUsed for categorical variables\nHave a specific label for each level of the variables\n\nE.g. Gender factors = male, female, non-binary, no_response\n\nThese can be dummy-coded as numbers but make sure you note what each number relates to\n\nE.g. Gender factors = male (0), female (1), non-binary (2), no_response (3)\n\n\nCharacter values\n\n Used for open-ended text responses\n\nE.g. Feedback on survey\n\nThese are typically manually coded outside of R\n\n\nIn order to run analyses, your data need to make sense. There are a few common annoyances that will break your code so fast, or worse… won’t break your code, but your results will make no sense.\nA couple that I run across a lot are:\n\nNumbers on a Likert scale that are saved as characters, so R can’t run any continuous analyses on them\nCategorical variables that are coded as character strings and need to be re-coded as factors so R can run categorical analyses on them\nA scale that has been re-coded by Qualtrics Gremlins so it no longer makes any reasonable sense (e.g. it should be 1(do not agree), 2, 3, 4, 5, 6, 7(do agree), but it’s now 10(do not agree), 2, 5, 6, 7, 1, 50(do agree)\n\nUsing your variable spreadsheet, systematically check that each variable is being read in R as the right type of variable. \nThere are several ways you can check this. For a long list of items I like to use the select() function. Here I’m checking on the structure of the 18 responses for the Well-being scale:\n# Checking on the structure of the well-being items\ndata_clean %&gt;% \n  select(WB_1_1_pre:WB_2_9_pre, WB_1_1_post:WB_2_9_post)\n\n# Code notes:\n# use : to indicate you want to select all the columns between these two variables\n# use , to make another selection\nThis prints out as:\n\n\n\n\n\n\n&gt; data_clean %&gt;%\n+   select(WB_1_1_pre:WB_2_9_pre, WB_1_1_post:WB_2_9_post)\n# A tibble: 6 × 38\n  WB_1_1_pre WB_1_2_pre WB_1_3_pre WB_1_4_pre WB_1_5_pre WB_1_6_pre WB_1_7_pre WB_1_8_pre WB_1_9_pre WB_2_1_pre\n  &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;    \n1 5          5          6          6          5          6          1          6          6          4        \n2 6          6          5          1          1          2          3          6          6          1        \n3 6          6          6          1          1          6          1          6          6          6        \n4 4          4          3          4          5          5          2          3          5          3        \n5 6          7          7          5          1          5          1          7          5          1        \n6 6          6          6          3          1          3          5          6          6          5        \n# … with 28 more variables: att_1_pre &lt;dbl&gt;, WB_2_2_pre &lt;chr&gt;, WB_2_3_pre &lt;chr&gt;, WB_2_4_pre &lt;chr&gt;,\n#   WB_2_5_pre &lt;chr&gt;, WB_2_6_pre &lt;chr&gt;, WB_2_7_pre &lt;chr&gt;, WB_2_8_pre &lt;chr&gt;, WB_2_9_pre &lt;chr&gt;, WB_1_1_post &lt;chr&gt;,\n#   WB_1_2_post &lt;chr&gt;, WB_1_3_post &lt;chr&gt;, WB_1_4_post &lt;chr&gt;, WB_1_5_post &lt;chr&gt;, WB_1_6_post &lt;chr&gt;,\n#   WB_1_7_post &lt;chr&gt;, WB_1_8_post &lt;chr&gt;, WB_1_9_post &lt;chr&gt;, WB_2_1_post &lt;chr&gt;, WB_2_2_post &lt;chr&gt;,\n#   WB_2_3_post &lt;chr&gt;, WB_2_4_post &lt;chr&gt;, att_1_post &lt;dbl&gt;, WB_2_5_post &lt;chr&gt;, WB_2_6_post &lt;chr&gt;,\n#   WB_2_7_post &lt;chr&gt;, WB_2_8_post &lt;chr&gt;, WB_2_9_post &lt;chr&gt;\n\n\n\nBecause there are so many variables here R is only going to print out the first 10, then it puts a comment at the bottom that gives a brief description.\nTo see what the variable is saved as, look at the description under the variable name in the print out (or next to it in the comment section at the bottom). \n\n&lt;dbl&gt; = numeric\n&lt;cha&gt; = character\n&lt;fac&gt; = factor\n\n\n\n\n\n\n\n# A tibble: 6 × 1\n  WB_1_1_pre\n  &lt;chr&gt;\n# … with 28 more variables: att_1_pre &lt;dbl&gt;, WB_2_2_pre &lt;chr&gt;, WB_2_3_pre &lt;chr&gt;,\n\n\n\n\nCharacter to Numeric Values\nTo change variables to numeric use the mutate() function and the as.numeric() function\n# Update them to numeric\ndata_clean &lt;-\n  data_clean %\\&gt;%\n  mutate(WB_1_1_pre = as.numeric(WB_1_1_pre),\n         WB_1_2_pre = as.numeric(WB_1_2_pre),\n         WB_1_3_pre = as.numeric(WB_1_3_pre),\n         WB_1_4_pre = as.numeric(WB_1_4_pre),\n         ... )            \n\n\nCharacter to Factor Variables\nTo change character variables to factors, use the mutate() function and the factor() function\n# recode as factor\ndata_clean &lt;-\n  data_clean %&gt;%\n  mutate(condition = factor(condition, levels = c(\"top\", \"bottom\", \"journal\")))\n\n# Code notes:\n# You can specify the order of the factors with levels. You will need to make a list using c() for this to work\n# If you want to rename your factors, use labels = c() to give them new names\nTo view your new factor and levels to make sure it makes sense use:\n\n\n\n\n\n\nlevels(data_clean$condition)\n[1] “top”     “bottom”  “journal”\n\n\n\nThis will show you the order of the levels as well, so you can make sure they make sense.\n\n\nRe-coding Scales\nIf you notice that there are some strange numbers in your scales and you need to recode them so they make sense with your original Likert scale, use the recode() function:\n#recode belief so it makes sense\n# 1 = 1, 2 = 2, 8 = 3, 9 = 4, 10 = 5\npilot_data_clean$belief &lt;-\n  recode(pilot_data_clean$belief, \"1\" = 1, \"2\" = 2, \"8\" = 3, \"9\" = 4, \"10\" = 5)\n\n# Code notes:\n# this works on the format: old_value = new_value\nYou should also check here whether you have properly reverse scored any items that needs to be coded that way.\n\n\n\nCreating aggregate scores (for specific measures)\nYou will now have a bunch of individual variables that need to be aggregated to make one total score for a measure.\nUse the mutate() function and simple mathematical signs to calculate these. Here I add up all the responses for the well-being measure for the pre test and the post test separately and divide them by the number of items in each measure. This gives a proportional score for the measure.\n# Create one score for Well-being Pre and wellbeing \npostdata_clean &lt;-\n  data_clean %&gt;%\n  mutate(WB_pre = (WB_1_1_pre + WB_1_2_pre + WB_1_3_pre + WB_1_4_pre + WB_1_5_pre + WB_1_6_pre + WB_1_7_pre + WB_1_8_pre + WB_1_9_pre +\n                     WB_2_1_pre + WB_2_2_pre + WB_2_3_pre + WB_2_4_pre + WB_2_5_pre + WB_2_6_pre + WB_2_7_pre + WB_2_8_pre + WB_2_9_pre)/18,\n         WB_post = (WB_1_1_post + WB_1_2_post + WB_1_3_post + WB_1_4_post + WB_1_5_post + WB_1_6_post + WB_1_7_post + WB_1_8_post + WB_1_9_post +\n                      WB_2_1_post + WB_2_2_post + WB_2_3_post + WB_2_4_post + WB_2_5_post + WB_2_6_post + WB_2_7_post + WB_2_8_post + WB_2_9_post)/18)\nSystematically go through each of your variables and create whatever aggregate score you need. Remember to give it a reasonable name so you can call it when you get to data analysis.\n\n\nOngoing Cleaning\n\n\n\n\n\nThere’s a very good chance that you’ll find things that need to still be cleaned up as you are running analyses. Rather than making those changes in your analysis scripts, go back and fix it in the cleaning document. Put the code in where it makes sense and re-run your cleaning script again. \nThe goal is to have your cleaning script do all the work of tidying up your data so your analysis scripts focus solely on calculating statistical test results.",
    "crumbs": [
      "Data Cleaning"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Getting Started With Data Analysis",
    "section": "",
    "text": "Beginning with data analysis can be really overwhelming. You have a bunch of responses on your survey but you don’t know what to do next. Here’s a step-by-step guide that can help you get started taking all those responses and turning them into something meaningful.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#what-this-guide-covers",
    "href": "index.html#what-this-guide-covers",
    "title": "Getting Started With Data Analysis",
    "section": "What this guide covers",
    "text": "What this guide covers\nThis guide will take you through the first basic steps to get familiar with your data and prepare for more detailed analyses. Other documents will go into more detail about those specific analyses that allow you to answer your research questions. \n\nGood Record Keeping\nDeveloping an Analysis Plan\nDownloading Data from Qualtrics\nCleaning Data (AKA Getting Data ready for analysis)",
    "crumbs": [
      "Overview"
    ]
  }
]